{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edd0a63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-11 20:58:10.682531: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-11 20:58:10.816589: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-04-11 20:58:11.363996: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-11 20:58:11.364080: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-11 20:58:11.364088: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "from hamp_pred.src.predictor import Predictor\n",
    "from hamp_pred.src.predictor_config import DEFAULT_CONF, PredictionConfig,SEQ_ENCODING_EXTERNAL\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import tensorflow as tf\n",
    "from hamp_pred.external.SequenceEncoding.SequenceEncoding import SequenceEncoding, get_dict\n",
    "from hamp_pred.src.input_prep.encode import OneHotEncoderSeq, RadianEncoder, MixedEncoder, MultiEncoder\n",
    "from hamp_pred.src.input_prep.prepare_sequence import MultiChainOperator, SeqWindow\n",
    "from hamp_pred.src.models.common.models import BaseConvolutionalWrapper, BaseLinearWrapper\n",
    "from hamp_pred.src.input_prep.encode import RadiousPhobosEncoder\n",
    "from hamp_pred.src.output_analysis.visual import reg_plot\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from Bio.PDB.PDBParser import PDBParser\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd966887",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e41b843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../../data/input'\n",
    "\n",
    "# data set contains train and validation sets\n",
    "# the val set is used for eraly stopping \n",
    "data = f'{DATA_DIR}/af2_clean_model_1.p'\n",
    "data = pd.read_pickle(data)\n",
    "data['n_seq'] = data['n_seq'].apply(lambda x: x[1:-1])\n",
    "data['c_seq'] = data['c_seq'].apply(lambda x: x[1:-1])\n",
    "data['train_seq'] = data.apply(lambda x: x['n_seq'] + x['c_seq'], axis=1)\n",
    "assert all(data['train_seq'].str.len() == 22)\n",
    "\n",
    "# separate test set used to pick the best model\n",
    "#data_test = f'{DATA_DIR}/af_clean_model_test.p'\n",
    "#data_test = pd.read_pickle(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f12335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    3978\n",
       "val       994\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fdae1e",
   "metadata": {},
   "source": [
    "## Train and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "690f1c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(val, mod):\n",
    "    seqs = list(val.train_seq)\n",
    "    tr=[]\n",
    "    for n,r in val.iterrows():\n",
    "        tr.append(np.mean((r['rot'][0::2] + r['rot'][1::2]) / 2)/2)\n",
    "    res = mod.predict(seqs)\n",
    "    pr = []\n",
    "    for n,r in res.iterrows():\n",
    "        pr.append(np.mean(r['N_pred'])/2)\n",
    "        \n",
    "    return np.mean((np.array(tr)-np.array(pr)) **2 ) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc8ef27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, conf, version):\n",
    "    operator = MultiChainOperator(MultiEncoder([RadiousPhobosEncoder(), OneHotEncoderSeq()]), SeqWindow(11, 11), RadianEncoder(100),  SeqWindow(11, 11, null_char=[[0]]),\n",
    "                                          parallel=True)\n",
    "    conf = PredictionConfig(BaseConvolutionalWrapper, operator, conf)\n",
    "    mod = Predictor('hamp_rot', config=conf, version=version)\n",
    "    trained = mod.train(data)\n",
    "    return mod, trained\n",
    "\n",
    "def get_mod(conf=None, version=None):\n",
    "    operator = MultiChainOperator(MultiEncoder([RadiousPhobosEncoder(), OneHotEncoderSeq()]), SeqWindow(11, 11), RadianEncoder(100),  SeqWindow(11, 11, null_char=[[0]]),\n",
    "                                          parallel=True)\n",
    "    conf = PredictionConfig(BaseConvolutionalWrapper, operator, conf)\n",
    "    mod = Predictor('hamp_rot', config=conf, version=version)\n",
    "    return mod\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "992c4f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_conf(data, tune=False):\n",
    "    \n",
    "    # data: train and val (val for the eraly stopping)\n",
    "    \n",
    "    data_val = data[data['class'] == 'val']\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    # parameters grid\n",
    "    kernels = [(3, 5, 7)]\n",
    "    layers = [3, 5, 7]\n",
    "    lstm = [1, 2]\n",
    "    dense = [1, 3]\n",
    "    \n",
    "    for kern in kernels:\n",
    "        for l in layers:\n",
    "            for ls in lstm:\n",
    "                for d in dense:\n",
    "                    model_config = {\n",
    "                        'activation': 'tanh',\n",
    "                        'norm': True,\n",
    "                        'n_layers': l,\n",
    "                        'kernel_sizes': kern,\n",
    "                        'lstm': ls,\n",
    "                        'dense': d,\n",
    "                        'reshape_out': False,\n",
    "                        'epochs': 100\n",
    "                    }\n",
    "                    version = '_'.join([str(x) for x in [\"_\".join([str(i) for i in kern]), l, ls, d]])\n",
    "                    if tune:\n",
    "                        mod, trained = train(data, model_config, version)\n",
    "                    else:\n",
    "                        mod = get_mod(model_config, version)\n",
    "                        \n",
    "                    results[version] = get_rmse(data_val, mod)\n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08447003",
   "metadata": {},
   "source": [
    "## Tune hiperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c640d33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-11 20:58:17.662290: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-11 20:58:18.210583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7380 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-11 20:58:22.767446: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8204\n",
      "2023-04-11 20:58:23.004109: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-04-11 20:58:23.004827: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-04-11 20:58:23.004884: W tensorflow/stream_executor/gpu/asm_compiler.cc:80] Couldn't get ptxas version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2023-04-11 20:58:23.005701: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-04-11 20:58:23.005805: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 7s 108ms/sample - loss: 0.0362 - mae: 0.1097 - val_loss: 0.0415 - val_mae: 0.1449 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 1s 24ms/sample - loss: 0.0147 - mae: 0.0751 - val_loss: 0.0176 - val_mae: 0.0845 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 1s 24ms/sample - loss: 0.0115 - mae: 0.0682 - val_loss: 0.0170 - val_mae: 0.0820 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 1s 23ms/sample - loss: 0.0082 - mae: 0.0586 - val_loss: 0.0165 - val_mae: 0.0778 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 1s 24ms/sample - loss: 0.0057 - mae: 0.0501 - val_loss: 0.0159 - val_mae: 0.0770 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0048 - mae: 0.0462 - val_loss: 0.0162 - val_mae: 0.0779 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0041 - mae: 0.0429 - val_loss: 0.0166 - val_mae: 0.0807 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 1s 23ms/sample - loss: 0.0040 - mae: 0.0431 - val_loss: 0.0155 - val_mae: 0.0750 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0039 - mae: 0.0422 - val_loss: 0.0188 - val_mae: 0.0885 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0035 - mae: 0.0399 - val_loss: 0.0177 - val_mae: 0.0796 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 2s 24ms/sample - loss: 0.0030 - mae: 0.0371 - val_loss: 0.0152 - val_mae: 0.0726 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0024 - mae: 0.0334 - val_loss: 0.0166 - val_mae: 0.0769 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 1s 24ms/sample - loss: 0.0024 - mae: 0.0336 - val_loss: 0.0148 - val_mae: 0.0736 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0022 - mae: 0.0319 - val_loss: 0.0152 - val_mae: 0.0750 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0019 - mae: 0.0299 - val_loss: 0.0149 - val_mae: 0.0731 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0018 - mae: 0.0290 - val_loss: 0.0153 - val_mae: 0.0730 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0018 - mae: 0.0293 - val_loss: 0.0152 - val_mae: 0.0748 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 1s 24ms/sample - loss: 0.0015 - mae: 0.0268 - val_loss: 0.0147 - val_mae: 0.0720 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0016 - mae: 0.0272 - val_loss: 0.0152 - val_mae: 0.0727 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 1s 24ms/sample - loss: 0.0016 - mae: 0.0275 - val_loss: 0.0146 - val_mae: 0.0719 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0015 - mae: 0.0262 - val_loss: 0.0149 - val_mae: 0.0731 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0016 - mae: 0.0270 - val_loss: 0.0147 - val_mae: 0.0724 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0017 - mae: 0.0277 - val_loss: 0.0147 - val_mae: 0.0727 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0014 - mae: 0.0259 - val_loss: 0.0151 - val_mae: 0.0743 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0014 - mae: 0.0257 - val_loss: 0.0149 - val_mae: 0.0717 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 1s 20ms/sample - loss: 0.0014 - mae: 0.0259 - val_loss: 0.0146 - val_mae: 0.0714 - lr: 0.0010\n",
      "Epoch 27/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 0.0013 - mae: 0.0243\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0013 - mae: 0.0244 - val_loss: 0.0152 - val_mae: 0.0724 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 2s 24ms/sample - loss: 9.6048e-04 - mae: 0.0213 - val_loss: 0.0141 - val_mae: 0.0699 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 6.6417e-04 - mae: 0.0182 - val_loss: 0.0141 - val_mae: 0.0699 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 6.0457e-04 - mae: 0.0176 - val_loss: 0.0141 - val_mae: 0.0700 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 5.0790e-04 - mae: 0.0162 - val_loss: 0.0142 - val_mae: 0.0701 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 5.0665e-04 - mae: 0.0163 - val_loss: 0.0142 - val_mae: 0.0701 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 4.2971e-04 - mae: 0.0150 - val_loss: 0.0143 - val_mae: 0.0699 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 4.2778e-04 - mae: 0.0150 - val_loss: 0.0142 - val_mae: 0.0699 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 4.0612e-04 - mae: 0.0147\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 4.0463e-04 - mae: 0.0147 - val_loss: 0.0142 - val_mae: 0.0701 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 3.6375e-04 - mae: 0.0140 - val_loss: 0.0142 - val_mae: 0.0699 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 3.6265e-04 - mae: 0.0139 - val_loss: 0.0142 - val_mae: 0.0699 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 3.6012e-04 - mae: 0.0139 - val_loss: 0.0142 - val_mae: 0.0698 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 3.5912e-04 - mae: 0.0138 - val_loss: 0.0143 - val_mae: 0.0698 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 3.6389e-04 - mae: 0.0140 - val_loss: 0.0142 - val_mae: 0.0698 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 3.5792e-04 - mae: 0.0139 - val_loss: 0.0142 - val_mae: 0.0698 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 3.7139e-04 - mae: 0.0141\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 3.7114e-04 - mae: 0.0141 - val_loss: 0.0142 - val_mae: 0.0699 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 3.5452e-04 - mae: 0.0138 - val_loss: 0.0142 - val_mae: 0.0699 - lr: 1.0000e-06\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 6s 93ms/sample - loss: 0.0361 - mae: 0.1104 - val_loss: 0.0246 - val_mae: 0.1028 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 2s 24ms/sample - loss: 0.0156 - mae: 0.0774 - val_loss: 0.0222 - val_mae: 0.0961 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 0.0108 - mae: 0.0651 - val_loss: 0.0165 - val_mae: 0.0770 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 1s 22ms/sample - loss: 0.0092 - mae: 0.0625 - val_loss: 0.0178 - val_mae: 0.0825 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0067 - mae: 0.0535 - val_loss: 0.0188 - val_mae: 0.0875 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 2s 24ms/sample - loss: 0.0053 - mae: 0.0484 - val_loss: 0.0156 - val_mae: 0.0756 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 0.0039 - mae: 0.0417 - val_loss: 0.0153 - val_mae: 0.0759 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0045 - mae: 0.0448 - val_loss: 0.0162 - val_mae: 0.0771 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 0.0031 - mae: 0.0370 - val_loss: 0.0149 - val_mae: 0.0718 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0028 - mae: 0.0355 - val_loss: 0.0168 - val_mae: 0.0804 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 1s 22ms/sample - loss: 0.0029 - mae: 0.0365 - val_loss: 0.0158 - val_mae: 0.0778 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 1s 22ms/sample - loss: 0.0030 - mae: 0.0362 - val_loss: 0.0154 - val_mae: 0.0727 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0022 - mae: 0.0315 - val_loss: 0.0150 - val_mae: 0.0736 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0021 - mae: 0.0308 - val_loss: 0.0152 - val_mae: 0.0719 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0022 - mae: 0.0321 - val_loss: 0.0162 - val_mae: 0.0785 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 2s 24ms/sample - loss: 0.0021 - mae: 0.0309 - val_loss: 0.0147 - val_mae: 0.0726 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 1s 22ms/sample - loss: 0.0021 - mae: 0.0309 - val_loss: 0.0157 - val_mae: 0.0729 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 2s 24ms/sample - loss: 0.0019 - mae: 0.0293 - val_loss: 0.0145 - val_mae: 0.0704 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 1s 22ms/sample - loss: 0.0015 - mae: 0.0259 - val_loss: 0.0152 - val_mae: 0.0742 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0018 - mae: 0.0284 - val_loss: 0.0149 - val_mae: 0.0738 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0019 - mae: 0.0296 - val_loss: 0.0148 - val_mae: 0.0726 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0017 - mae: 0.0275 - val_loss: 0.0148 - val_mae: 0.0726 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0013 - mae: 0.0248 - val_loss: 0.0150 - val_mae: 0.0711 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0013 - mae: 0.0243 - val_loss: 0.0147 - val_mae: 0.0706 - lr: 0.0010\n",
      "Epoch 25/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 0.0011 - mae: 0.0223\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 0.0011 - mae: 0.0224 - val_loss: 0.0154 - val_mae: 0.0716 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 2s 24ms/sample - loss: 8.0504e-04 - mae: 0.0194 - val_loss: 0.0142 - val_mae: 0.0687 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 2s 24ms/sample - loss: 5.5532e-04 - mae: 0.0164 - val_loss: 0.0142 - val_mae: 0.0687 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 2s 24ms/sample - loss: 4.6880e-04 - mae: 0.0152 - val_loss: 0.0141 - val_mae: 0.0689 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 4.2165e-04 - mae: 0.0144 - val_loss: 0.0141 - val_mae: 0.0687 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 4.0649e-04 - mae: 0.0142 - val_loss: 0.0141 - val_mae: 0.0690 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 1s 22ms/sample - loss: 3.8127e-04 - mae: 0.0138 - val_loss: 0.0141 - val_mae: 0.0687 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 3.6965e-04 - mae: 0.0136 - val_loss: 0.0142 - val_mae: 0.0686 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 1s 22ms/sample - loss: 3.6094e-04 - mae: 0.0135 - val_loss: 0.0141 - val_mae: 0.0687 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 3.6382e-04 - mae: 0.0135 - val_loss: 0.0144 - val_mae: 0.0689 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 3.2479e-04 - mae: 0.0128\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "63/63 [==============================] - 1s 22ms/sample - loss: 3.2596e-04 - mae: 0.0129 - val_loss: 0.0141 - val_mae: 0.0691 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 3.0278e-04 - mae: 0.0124 - val_loss: 0.0141 - val_mae: 0.0687 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 1s 22ms/sample - loss: 3.0189e-04 - mae: 0.0124 - val_loss: 0.0142 - val_mae: 0.0687 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 1s 22ms/sample - loss: 2.8683e-04 - mae: 0.0121 - val_loss: 0.0141 - val_mae: 0.0687 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 1s 22ms/sample - loss: 2.9822e-04 - mae: 0.0123 - val_loss: 0.0141 - val_mae: 0.0687 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 3.0840e-04 - mae: 0.0125 - val_loss: 0.0141 - val_mae: 0.0687 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 2.8940e-04 - mae: 0.0122 - val_loss: 0.0141 - val_mae: 0.0687 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 2.9661e-04 - mae: 0.0123\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 2.9777e-04 - mae: 0.0123 - val_loss: 0.0142 - val_mae: 0.0686 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 1s 22ms/sample - loss: 3.1032e-04 - mae: 0.0126 - val_loss: 0.0141 - val_mae: 0.0687 - lr: 1.0000e-06\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 1s 22ms/sample - loss: 2.9160e-04 - mae: 0.0122 - val_loss: 0.0141 - val_mae: 0.0687 - lr: 1.0000e-06\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 1s 21ms/sample - loss: 2.8958e-04 - mae: 0.0122 - val_loss: 0.0141 - val_mae: 0.0687 - lr: 1.0000e-06\n",
      "1/1 [==============================] - 1s 801ms/step\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 9s 137ms/sample - loss: 0.0368 - mae: 0.1120 - val_loss: 0.0225 - val_mae: 0.0935 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0148 - mae: 0.0749 - val_loss: 0.0171 - val_mae: 0.0784 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0116 - mae: 0.0684 - val_loss: 0.0170 - val_mae: 0.0796 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 0.0079 - mae: 0.0571 - val_loss: 0.0188 - val_mae: 0.0870 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 2s 29ms/sample - loss: 0.0068 - mae: 0.0538 - val_loss: 0.0169 - val_mae: 0.0827 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 2s 29ms/sample - loss: 0.0053 - mae: 0.0482 - val_loss: 0.0164 - val_mae: 0.0772 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0046 - mae: 0.0453 - val_loss: 0.0163 - val_mae: 0.0770 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 2s 29ms/sample - loss: 0.0038 - mae: 0.0411 - val_loss: 0.0153 - val_mae: 0.0751 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 0.0035 - mae: 0.0400 - val_loss: 0.0154 - val_mae: 0.0744 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 0.0031 - mae: 0.0369 - val_loss: 0.0172 - val_mae: 0.0813 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0030 - mae: 0.0368 - val_loss: 0.0152 - val_mae: 0.0740 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0023 - mae: 0.0326 - val_loss: 0.0149 - val_mae: 0.0740 - lr: 0.0010\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 2s 25ms/sample - loss: 0.0029 - mae: 0.0363 - val_loss: 0.0168 - val_mae: 0.0771 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 0.0023 - mae: 0.0322 - val_loss: 0.0152 - val_mae: 0.0733 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0018 - mae: 0.0287 - val_loss: 0.0147 - val_mae: 0.0719 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0017 - mae: 0.0276 - val_loss: 0.0145 - val_mae: 0.0721 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 0.0016 - mae: 0.0274 - val_loss: 0.0155 - val_mae: 0.0764 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 0.0016 - mae: 0.0272 - val_loss: 0.0148 - val_mae: 0.0721 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 0.0013 - mae: 0.0247 - val_loss: 0.0147 - val_mae: 0.0713 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 0.0013 - mae: 0.0245 - val_loss: 0.0151 - val_mae: 0.0727 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 0.0011 - mae: 0.0230 - val_loss: 0.0148 - val_mae: 0.0715 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 0.0012 - mae: 0.0237 - val_loss: 0.0149 - val_mae: 0.0720 - lr: 0.0010\n",
      "Epoch 23/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 0.0013 - mae: 0.0240\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 0.0013 - mae: 0.0240 - val_loss: 0.0148 - val_mae: 0.0711 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 9.7155e-04 - mae: 0.0210 - val_loss: 0.0144 - val_mae: 0.0702 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 7.5386e-04 - mae: 0.0187 - val_loss: 0.0142 - val_mae: 0.0702 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 6.5818e-04 - mae: 0.0176 - val_loss: 0.0144 - val_mae: 0.0701 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 2s 29ms/sample - loss: 6.0126e-04 - mae: 0.0169 - val_loss: 0.0142 - val_mae: 0.0701 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 5.5678e-04 - mae: 0.0163 - val_loss: 0.0144 - val_mae: 0.0700 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 5.0266e-04 - mae: 0.0155 - val_loss: 0.0144 - val_mae: 0.0700 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 4.6608e-04 - mae: 0.0149 - val_loss: 0.0142 - val_mae: 0.0700 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 4.6842e-04 - mae: 0.0149 - val_loss: 0.0144 - val_mae: 0.0700 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 4.7418e-04 - mae: 0.0150\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 4.7416e-04 - mae: 0.0150 - val_loss: 0.0142 - val_mae: 0.0701 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 4.5241e-04 - mae: 0.0147 - val_loss: 0.0143 - val_mae: 0.0699 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 4.4235e-04 - mae: 0.0145 - val_loss: 0.0143 - val_mae: 0.0699 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 4.2373e-04 - mae: 0.0143 - val_loss: 0.0143 - val_mae: 0.0699 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 4.2480e-04 - mae: 0.0143 - val_loss: 0.0143 - val_mae: 0.0699 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 4.2219e-04 - mae: 0.0142 - val_loss: 0.0143 - val_mae: 0.0699 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 4.0998e-04 - mae: 0.0140 - val_loss: 0.0143 - val_mae: 0.0700 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 4.3130e-04 - mae: 0.0144\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 4.3630e-04 - mae: 0.0144 - val_loss: 0.0143 - val_mae: 0.0700 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 4.0706e-04 - mae: 0.0140 - val_loss: 0.0143 - val_mae: 0.0699 - lr: 1.0000e-06\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 4.3256e-04 - mae: 0.0144 - val_loss: 0.0143 - val_mae: 0.0699 - lr: 1.0000e-06\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 4.1103e-04 - mae: 0.0140 - val_loss: 0.0143 - val_mae: 0.0699 - lr: 1.0000e-06\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 9s 141ms/sample - loss: 0.0428 - mae: 0.1210 - val_loss: 0.0307 - val_mae: 0.1162 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 2s 29ms/sample - loss: 0.0154 - mae: 0.0761 - val_loss: 0.0222 - val_mae: 0.0946 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 2s 29ms/sample - loss: 0.0121 - mae: 0.0694 - val_loss: 0.0180 - val_mae: 0.0867 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 2s 29ms/sample - loss: 0.0086 - mae: 0.0599 - val_loss: 0.0165 - val_mae: 0.0782 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 0.0066 - mae: 0.0537 - val_loss: 0.0174 - val_mae: 0.0797 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 2s 29ms/sample - loss: 0.0059 - mae: 0.0500 - val_loss: 0.0156 - val_mae: 0.0756 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 0.0054 - mae: 0.0484 - val_loss: 0.0159 - val_mae: 0.0759 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 0.0043 - mae: 0.0437 - val_loss: 0.0165 - val_mae: 0.0774 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 0.0038 - mae: 0.0414 - val_loss: 0.0168 - val_mae: 0.0768 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0028 - mae: 0.0351 - val_loss: 0.0155 - val_mae: 0.0761 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 0.0030 - mae: 0.0370 - val_loss: 0.0168 - val_mae: 0.0812 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0024 - mae: 0.0330 - val_loss: 0.0151 - val_mae: 0.0740 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 2s 29ms/sample - loss: 0.0019 - mae: 0.0293 - val_loss: 0.0147 - val_mae: 0.0723 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 0.0021 - mae: 0.0308 - val_loss: 0.0156 - val_mae: 0.0734 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 0.0018 - mae: 0.0283 - val_loss: 0.0150 - val_mae: 0.0718 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 0.0018 - mae: 0.0285 - val_loss: 0.0165 - val_mae: 0.0754 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 0.0021 - mae: 0.0309 - val_loss: 0.0152 - val_mae: 0.0727 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 0.0022 - mae: 0.0314 - val_loss: 0.0158 - val_mae: 0.0742 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 0.0021 - mae: 0.0309 - val_loss: 0.0153 - val_mae: 0.0728 - lr: 0.0010\n",
      "Epoch 20/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 0.0020 - mae: 0.0295\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 0.0021 - mae: 0.0296 - val_loss: 0.0147 - val_mae: 0.0712 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "63/63 [==============================] - 2s 29ms/sample - loss: 0.0013 - mae: 0.0244 - val_loss: 0.0143 - val_mae: 0.0703 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 9.6382e-04 - mae: 0.0209 - val_loss: 0.0143 - val_mae: 0.0699 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 8.5462e-04 - mae: 0.0196 - val_loss: 0.0143 - val_mae: 0.0698 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 7.4762e-04 - mae: 0.0184 - val_loss: 0.0143 - val_mae: 0.0700 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 7.2961e-04 - mae: 0.0183 - val_loss: 0.0144 - val_mae: 0.0698 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 6.5467e-04 - mae: 0.0173 - val_loss: 0.0145 - val_mae: 0.0700 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 2s 25ms/sample - loss: 6.4630e-04 - mae: 0.0173 - val_loss: 0.0143 - val_mae: 0.0700 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 6.0068e-04 - mae: 0.0167\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 6.0541e-04 - mae: 0.0167 - val_loss: 0.0145 - val_mae: 0.0700 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 5.5780e-04 - mae: 0.0161 - val_loss: 0.0144 - val_mae: 0.0700 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 5.3898e-04 - mae: 0.0157 - val_loss: 0.0144 - val_mae: 0.0700 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 5.6204e-04 - mae: 0.0161 - val_loss: 0.0144 - val_mae: 0.0700 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 5.6365e-04 - mae: 0.0161 - val_loss: 0.0144 - val_mae: 0.0700 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 5.3856e-04 - mae: 0.0158 - val_loss: 0.0145 - val_mae: 0.0699 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 5.5884e-04 - mae: 0.0161 - val_loss: 0.0144 - val_mae: 0.0700 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 5.7173e-04 - mae: 0.0162\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 5.7312e-04 - mae: 0.0162 - val_loss: 0.0144 - val_mae: 0.0702 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 2s 26ms/sample - loss: 5.5792e-04 - mae: 0.0161 - val_loss: 0.0144 - val_mae: 0.0701 - lr: 1.0000e-06\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 7s 112ms/sample - loss: 0.0349 - mae: 0.1074 - val_loss: 0.0215 - val_mae: 0.0960 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0143 - mae: 0.0746 - val_loss: 0.0186 - val_mae: 0.0853 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0112 - mae: 0.0681 - val_loss: 0.0164 - val_mae: 0.0790 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0080 - mae: 0.0585 - val_loss: 0.0163 - val_mae: 0.0789 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0066 - mae: 0.0543 - val_loss: 0.0170 - val_mae: 0.0793 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0053 - mae: 0.0484 - val_loss: 0.0166 - val_mae: 0.0782 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0048 - mae: 0.0463 - val_loss: 0.0163 - val_mae: 0.0775 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0042 - mae: 0.0438 - val_loss: 0.0156 - val_mae: 0.0766 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0035 - mae: 0.0392 - val_loss: 0.0155 - val_mae: 0.0751 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0033 - mae: 0.0386 - val_loss: 0.0154 - val_mae: 0.0746 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0025 - mae: 0.0336 - val_loss: 0.0166 - val_mae: 0.0749 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0026 - mae: 0.0344 - val_loss: 0.0161 - val_mae: 0.0759 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0026 - mae: 0.0350 - val_loss: 0.0166 - val_mae: 0.0757 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0027 - mae: 0.0348 - val_loss: 0.0161 - val_mae: 0.0750 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0022 - mae: 0.0310 - val_loss: 0.0159 - val_mae: 0.0772 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 2s 30ms/sample - loss: 0.0020 - mae: 0.0308 - val_loss: 0.0146 - val_mae: 0.0709 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0014 - mae: 0.0256 - val_loss: 0.0148 - val_mae: 0.0725 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0018 - mae: 0.0289 - val_loss: 0.0148 - val_mae: 0.0708 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0019 - mae: 0.0296 - val_loss: 0.0150 - val_mae: 0.0717 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0018 - mae: 0.0288 - val_loss: 0.0154 - val_mae: 0.0763 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0018 - mae: 0.0289 - val_loss: 0.0146 - val_mae: 0.0709 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0015 - mae: 0.0263 - val_loss: 0.0146 - val_mae: 0.0702 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0015 - mae: 0.0264 - val_loss: 0.0145 - val_mae: 0.0703 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0016 - mae: 0.0268 - val_loss: 0.0150 - val_mae: 0.0706 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0017 - mae: 0.0279 - val_loss: 0.0153 - val_mae: 0.0743 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0016 - mae: 0.0275 - val_loss: 0.0157 - val_mae: 0.0728 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0017 - mae: 0.0280 - val_loss: 0.0141 - val_mae: 0.0695 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0014 - mae: 0.0253 - val_loss: 0.0146 - val_mae: 0.0702 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0015 - mae: 0.0261 - val_loss: 0.0148 - val_mae: 0.0707 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0012 - mae: 0.0229 - val_loss: 0.0143 - val_mae: 0.0691 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0011 - mae: 0.0223 - val_loss: 0.0140 - val_mae: 0.0688 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0013 - mae: 0.0240 - val_loss: 0.0146 - val_mae: 0.0724 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0011 - mae: 0.0224 - val_loss: 0.0141 - val_mae: 0.0696 - lr: 0.0010\n",
      "Epoch 34/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 0.0011 - mae: 0.0227\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0011 - mae: 0.0227 - val_loss: 0.0143 - val_mae: 0.0695 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 7.4807e-04 - mae: 0.0187 - val_loss: 0.0137 - val_mae: 0.0676 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 5.3108e-04 - mae: 0.0160 - val_loss: 0.0137 - val_mae: 0.0675 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 4.7597e-04 - mae: 0.0152 - val_loss: 0.0138 - val_mae: 0.0674 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 4.1269e-04 - mae: 0.0143 - val_loss: 0.0137 - val_mae: 0.0677 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 3.8112e-04 - mae: 0.0139 - val_loss: 0.0138 - val_mae: 0.0675 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 3.4320e-04 - mae: 0.0132 - val_loss: 0.0139 - val_mae: 0.0675 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 3.2545e-04 - mae: 0.0129 - val_loss: 0.0137 - val_mae: 0.0677 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 3.1403e-04 - mae: 0.0127\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 3.1757e-04 - mae: 0.0127 - val_loss: 0.0138 - val_mae: 0.0675 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 2.7797e-04 - mae: 0.0120 - val_loss: 0.0138 - val_mae: 0.0674 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 2.9892e-04 - mae: 0.0124 - val_loss: 0.0138 - val_mae: 0.0675 - lr: 1.0000e-05\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 2.7566e-04 - mae: 0.0119 - val_loss: 0.0138 - val_mae: 0.0674 - lr: 1.0000e-05\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 3.3031e-04 - mae: 0.0130 - val_loss: 0.0138 - val_mae: 0.0675 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 2.8498e-04 - mae: 0.0121 - val_loss: 0.0138 - val_mae: 0.0674 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 2.7656e-04 - mae: 0.0119 - val_loss: 0.0138 - val_mae: 0.0674 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 2.5994e-04 - mae: 0.0116\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 2.5994e-04 - mae: 0.0116 - val_loss: 0.0138 - val_mae: 0.0674 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 2.5656e-04 - mae: 0.0116 - val_loss: 0.0138 - val_mae: 0.0674 - lr: 1.0000e-06\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd6decfd120> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 843ms/step\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 7s 115ms/sample - loss: 0.0354 - mae: 0.1071 - val_loss: 0.0214 - val_mae: 0.0910 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 0.0160 - mae: 0.0791 - val_loss: 0.0260 - val_mae: 0.1046 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0120 - mae: 0.0699 - val_loss: 0.0157 - val_mae: 0.0771 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0086 - mae: 0.0596 - val_loss: 0.0230 - val_mae: 0.1057 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0080 - mae: 0.0596 - val_loss: 0.0168 - val_mae: 0.0798 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0053 - mae: 0.0481 - val_loss: 0.0159 - val_mae: 0.0762 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0049 - mae: 0.0468 - val_loss: 0.0174 - val_mae: 0.0790 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0040 - mae: 0.0420 - val_loss: 0.0210 - val_mae: 0.0880 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0044 - mae: 0.0440 - val_loss: 0.0153 - val_mae: 0.0753 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0037 - mae: 0.0407 - val_loss: 0.0161 - val_mae: 0.0743 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0032 - mae: 0.0375 - val_loss: 0.0154 - val_mae: 0.0740 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0027 - mae: 0.0350 - val_loss: 0.0155 - val_mae: 0.0727 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0036 - mae: 0.0405 - val_loss: 0.0145 - val_mae: 0.0712 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0026 - mae: 0.0342 - val_loss: 0.0146 - val_mae: 0.0717 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0021 - mae: 0.0306 - val_loss: 0.0149 - val_mae: 0.0716 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0024 - mae: 0.0335 - val_loss: 0.0173 - val_mae: 0.0782 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0025 - mae: 0.0340 - val_loss: 0.0177 - val_mae: 0.0827 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0034 - mae: 0.0393 - val_loss: 0.0171 - val_mae: 0.0766 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0025 - mae: 0.0331 - val_loss: 0.0147 - val_mae: 0.0710 - lr: 0.0010\n",
      "Epoch 20/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 0.0051 - mae: 0.0464\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0051 - mae: 0.0463 - val_loss: 0.0171 - val_mae: 0.0787 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 0.0021 - mae: 0.0310 - val_loss: 0.0142 - val_mae: 0.0690 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 0.0014 - mae: 0.0252 - val_loss: 0.0143 - val_mae: 0.0691 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0011 - mae: 0.0225 - val_loss: 0.0142 - val_mae: 0.0691 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 9.8408e-04 - mae: 0.0214 - val_loss: 0.0141 - val_mae: 0.0690 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 8.9645e-04 - mae: 0.0206 - val_loss: 0.0143 - val_mae: 0.0689 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 8.4118e-04 - mae: 0.0199 - val_loss: 0.0142 - val_mae: 0.0689 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 7.8023e-04 - mae: 0.0193 - val_loss: 0.0142 - val_mae: 0.0688 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 7.1216e-04 - mae: 0.0184\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 7.1216e-04 - mae: 0.0184 - val_loss: 0.0141 - val_mae: 0.0691 - lr: 1.0000e-04\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 2s 28ms/sample - loss: 6.5812e-04 - mae: 0.0178 - val_loss: 0.0142 - val_mae: 0.0688 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 6.3473e-04 - mae: 0.0175 - val_loss: 0.0142 - val_mae: 0.0688 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 6.4382e-04 - mae: 0.0175 - val_loss: 0.0142 - val_mae: 0.0688 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 6.8820e-04 - mae: 0.0182 - val_loss: 0.0142 - val_mae: 0.0689 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 6.2342e-04 - mae: 0.0173 - val_loss: 0.0142 - val_mae: 0.0689 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 6.4468e-04 - mae: 0.0177 - val_loss: 0.0142 - val_mae: 0.0689 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 6.1361e-04 - mae: 0.0172\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 6.1415e-04 - mae: 0.0172 - val_loss: 0.0142 - val_mae: 0.0689 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 2s 27ms/sample - loss: 5.9979e-04 - mae: 0.0171 - val_loss: 0.0142 - val_mae: 0.0689 - lr: 1.0000e-06\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 5.9975e-04 - mae: 0.0170 - val_loss: 0.0142 - val_mae: 0.0689 - lr: 1.0000e-06\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 6.0213e-04 - mae: 0.0171 - val_loss: 0.0142 - val_mae: 0.0689 - lr: 1.0000e-06\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 2s 28ms/sample - loss: 5.9230e-04 - mae: 0.0169 - val_loss: 0.0142 - val_mae: 0.0689 - lr: 1.0000e-06\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd6dd25cb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 893ms/step\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 10s 156ms/sample - loss: 0.0356 - mae: 0.1080 - val_loss: 0.0238 - val_mae: 0.0981 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 2s 35ms/sample - loss: 0.0133 - mae: 0.0713 - val_loss: 0.0179 - val_mae: 0.0825 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 2s 35ms/sample - loss: 0.0116 - mae: 0.0688 - val_loss: 0.0167 - val_mae: 0.0786 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 0.0089 - mae: 0.0613 - val_loss: 0.0253 - val_mae: 0.1034 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 0.0082 - mae: 0.0601 - val_loss: 0.0209 - val_mae: 0.0893 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 2s 35ms/sample - loss: 0.0062 - mae: 0.0522 - val_loss: 0.0167 - val_mae: 0.0776 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 2s 35ms/sample - loss: 0.0044 - mae: 0.0439 - val_loss: 0.0150 - val_mae: 0.0727 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0043 - mae: 0.0431 - val_loss: 0.0154 - val_mae: 0.0753 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 2s 36ms/sample - loss: 0.0033 - mae: 0.0380 - val_loss: 0.0149 - val_mae: 0.0721 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 2s 35ms/sample - loss: 0.0027 - mae: 0.0346 - val_loss: 0.0149 - val_mae: 0.0723 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0027 - mae: 0.0346 - val_loss: 0.0160 - val_mae: 0.0777 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 2s 36ms/sample - loss: 0.0032 - mae: 0.0384 - val_loss: 0.0148 - val_mae: 0.0715 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0029 - mae: 0.0364 - val_loss: 0.0149 - val_mae: 0.0727 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0026 - mae: 0.0337 - val_loss: 0.0155 - val_mae: 0.0741 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0025 - mae: 0.0335 - val_loss: 0.0151 - val_mae: 0.0713 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 2s 35ms/sample - loss: 0.0018 - mae: 0.0288 - val_loss: 0.0142 - val_mae: 0.0693 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0014 - mae: 0.0253 - val_loss: 0.0153 - val_mae: 0.0743 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0022 - mae: 0.0315 - val_loss: 0.0152 - val_mae: 0.0719 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0020 - mae: 0.0307 - val_loss: 0.0148 - val_mae: 0.0711 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0020 - mae: 0.0291 - val_loss: 0.0143 - val_mae: 0.0706 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0018 - mae: 0.0288 - val_loss: 0.0156 - val_mae: 0.0724 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0015 - mae: 0.0259 - val_loss: 0.0145 - val_mae: 0.0698 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 2s 35ms/sample - loss: 0.0012 - mae: 0.0237 - val_loss: 0.0140 - val_mae: 0.0695 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 0.0014 - mae: 0.0256 - val_loss: 0.0152 - val_mae: 0.0747 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0012 - mae: 0.0230 - val_loss: 0.0147 - val_mae: 0.0700 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 2s 35ms/sample - loss: 0.0012 - mae: 0.0234 - val_loss: 0.0140 - val_mae: 0.0686 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 2s 35ms/sample - loss: 0.0011 - mae: 0.0226 - val_loss: 0.0138 - val_mae: 0.0683 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0011 - mae: 0.0225 - val_loss: 0.0140 - val_mae: 0.0693 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0011 - mae: 0.0223 - val_loss: 0.0141 - val_mae: 0.0689 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0012 - mae: 0.0231 - val_loss: 0.0142 - val_mae: 0.0691 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0011 - mae: 0.0225 - val_loss: 0.0142 - val_mae: 0.0683 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0011 - mae: 0.0224 - val_loss: 0.0163 - val_mae: 0.0740 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0018 - mae: 0.0286 - val_loss: 0.0152 - val_mae: 0.0713 - lr: 0.0010\n",
      "Epoch 34/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 0.0013 - mae: 0.0245\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 0.0013 - mae: 0.0245 - val_loss: 0.0140 - val_mae: 0.0689 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 2s 35ms/sample - loss: 8.1200e-04 - mae: 0.0189 - val_loss: 0.0138 - val_mae: 0.0679 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 6.4025e-04 - mae: 0.0170 - val_loss: 0.0138 - val_mae: 0.0675 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100\n",
      "63/63 [==============================] - 2s 35ms/sample - loss: 5.4495e-04 - mae: 0.0158 - val_loss: 0.0137 - val_mae: 0.0674 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 2s 35ms/sample - loss: 4.3793e-04 - mae: 0.0143 - val_loss: 0.0137 - val_mae: 0.0674 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 4.2201e-04 - mae: 0.0140 - val_loss: 0.0139 - val_mae: 0.0676 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 3.9048e-04 - mae: 0.0135 - val_loss: 0.0137 - val_mae: 0.0676 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 3.3858e-04 - mae: 0.0126 - val_loss: 0.0140 - val_mae: 0.0677 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 3.7771e-04 - mae: 0.0134 - val_loss: 0.0138 - val_mae: 0.0674 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 2s 35ms/sample - loss: 3.3317e-04 - mae: 0.0126 - val_loss: 0.0137 - val_mae: 0.0674 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 3.1627e-04 - mae: 0.0122 - val_loss: 0.0138 - val_mae: 0.0675 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 3.3841e-04 - mae: 0.0126\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 3.3864e-04 - mae: 0.0126 - val_loss: 0.0137 - val_mae: 0.0674 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 2.6201e-04 - mae: 0.0112 - val_loss: 0.0137 - val_mae: 0.0673 - lr: 1.0000e-05\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 2.7796e-04 - mae: 0.0115 - val_loss: 0.0137 - val_mae: 0.0674 - lr: 1.0000e-05\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 2.7991e-04 - mae: 0.0116 - val_loss: 0.0138 - val_mae: 0.0673 - lr: 1.0000e-05\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 2.8783e-04 - mae: 0.0117 - val_loss: 0.0137 - val_mae: 0.0674 - lr: 1.0000e-05\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 2.9866e-04 - mae: 0.0119 - val_loss: 0.0137 - val_mae: 0.0673 - lr: 1.0000e-05\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 2.7751e-04 - mae: 0.0115 - val_loss: 0.0137 - val_mae: 0.0673 - lr: 1.0000e-05\n",
      "Epoch 52/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 3.0868e-04 - mae: 0.0121\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 3.0868e-04 - mae: 0.0121 - val_loss: 0.0137 - val_mae: 0.0674 - lr: 1.0000e-05\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 2.9578e-04 - mae: 0.0118 - val_loss: 0.0137 - val_mae: 0.0674 - lr: 1.0000e-06\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 2.6979e-04 - mae: 0.0114 - val_loss: 0.0137 - val_mae: 0.0674 - lr: 1.0000e-06\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 2.8449e-04 - mae: 0.0116 - val_loss: 0.0137 - val_mae: 0.0673 - lr: 1.0000e-06\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 2.7634e-04 - mae: 0.0115 - val_loss: 0.0137 - val_mae: 0.0674 - lr: 1.0000e-06\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 2.8742e-04 - mae: 0.0117 - val_loss: 0.0137 - val_mae: 0.0673 - lr: 1.0000e-06\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 2s 31ms/sample - loss: 2.6548e-04 - mae: 0.0112 - val_loss: 0.0137 - val_mae: 0.0673 - lr: 1.0000e-06\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 10s 157ms/sample - loss: 0.0384 - mae: 0.1130 - val_loss: 0.0247 - val_mae: 0.1017 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 2s 36ms/sample - loss: 0.0146 - mae: 0.0748 - val_loss: 0.0174 - val_mae: 0.0823 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 2s 36ms/sample - loss: 0.0096 - mae: 0.0630 - val_loss: 0.0158 - val_mae: 0.0769 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 0.0081 - mae: 0.0585 - val_loss: 0.0161 - val_mae: 0.0788 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 0.0073 - mae: 0.0562 - val_loss: 0.0172 - val_mae: 0.0799 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 0.0052 - mae: 0.0475 - val_loss: 0.0159 - val_mae: 0.0760 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 0.0048 - mae: 0.0463 - val_loss: 0.0162 - val_mae: 0.0739 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 0.0044 - mae: 0.0436 - val_loss: 0.0169 - val_mae: 0.0768 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 0.0043 - mae: 0.0440 - val_loss: 0.0169 - val_mae: 0.0811 - lr: 0.0010\n",
      "Epoch 10/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 0.0039 - mae: 0.0420\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 0.0039 - mae: 0.0420 - val_loss: 0.0174 - val_mae: 0.0776 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 2s 36ms/sample - loss: 0.0024 - mae: 0.0330 - val_loss: 0.0143 - val_mae: 0.0707 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0017 - mae: 0.0278 - val_loss: 0.0143 - val_mae: 0.0708 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 0.0015 - mae: 0.0264 - val_loss: 0.0143 - val_mae: 0.0698 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 2s 36ms/sample - loss: 0.0014 - mae: 0.0250 - val_loss: 0.0142 - val_mae: 0.0703 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 0.0013 - mae: 0.0243 - val_loss: 0.0143 - val_mae: 0.0709 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 0.0013 - mae: 0.0245 - val_loss: 0.0143 - val_mae: 0.0700 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 2s 33ms/sample - loss: 0.0011 - mae: 0.0229 - val_loss: 0.0143 - val_mae: 0.0700 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 0.0010 - mae: 0.0220 - val_loss: 0.0145 - val_mae: 0.0699 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 9.8157e-04 - mae: 0.0215 - val_loss: 0.0144 - val_mae: 0.0699 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 9.4594e-04 - mae: 0.0212 - val_loss: 0.0143 - val_mae: 0.0700 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 9.1466e-04 - mae: 0.0207\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 9.1472e-04 - mae: 0.0207 - val_loss: 0.0144 - val_mae: 0.0701 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 8.1383e-04 - mae: 0.0196 - val_loss: 0.0143 - val_mae: 0.0699 - lr: 1.0000e-05\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 7.9111e-04 - mae: 0.0193 - val_loss: 0.0144 - val_mae: 0.0698 - lr: 1.0000e-05\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 8.2836e-04 - mae: 0.0198 - val_loss: 0.0143 - val_mae: 0.0699 - lr: 1.0000e-05\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 7.9144e-04 - mae: 0.0193 - val_loss: 0.0145 - val_mae: 0.0698 - lr: 1.0000e-05\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 8.0061e-04 - mae: 0.0194 - val_loss: 0.0143 - val_mae: 0.0700 - lr: 1.0000e-05\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 8.3450e-04 - mae: 0.0198 - val_loss: 0.0143 - val_mae: 0.0699 - lr: 1.0000e-05\n",
      "Epoch 28/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 8.1341e-04 - mae: 0.0196\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "63/63 [==============================] - 2s 33ms/sample - loss: 8.1341e-04 - mae: 0.0196 - val_loss: 0.0144 - val_mae: 0.0698 - lr: 1.0000e-05\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 2s 32ms/sample - loss: 7.9367e-04 - mae: 0.0194 - val_loss: 0.0144 - val_mae: 0.0698 - lr: 1.0000e-06\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 8s 127ms/sample - loss: 0.0343 - mae: 0.1063 - val_loss: 0.0245 - val_mae: 0.1028 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 3s 41ms/sample - loss: 0.0147 - mae: 0.0756 - val_loss: 0.0175 - val_mae: 0.0828 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 2s 33ms/sample - loss: 0.0106 - mae: 0.0647 - val_loss: 0.0205 - val_mae: 0.0952 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0080 - mae: 0.0570 - val_loss: 0.0172 - val_mae: 0.0772 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0067 - mae: 0.0538 - val_loss: 0.0182 - val_mae: 0.0833 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0066 - mae: 0.0533 - val_loss: 0.0161 - val_mae: 0.0779 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0049 - mae: 0.0462 - val_loss: 0.0151 - val_mae: 0.0726 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0038 - mae: 0.0407 - val_loss: 0.0154 - val_mae: 0.0743 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 2s 33ms/sample - loss: 0.0041 - mae: 0.0431 - val_loss: 0.0170 - val_mae: 0.0796 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 2s 33ms/sample - loss: 0.0039 - mae: 0.0422 - val_loss: 0.0152 - val_mae: 0.0728 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0029 - mae: 0.0360 - val_loss: 0.0169 - val_mae: 0.0805 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0028 - mae: 0.0352 - val_loss: 0.0145 - val_mae: 0.0721 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 2s 33ms/sample - loss: 0.0024 - mae: 0.0332 - val_loss: 0.0149 - val_mae: 0.0725 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0022 - mae: 0.0318 - val_loss: 0.0145 - val_mae: 0.0703 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0022 - mae: 0.0318 - val_loss: 0.0159 - val_mae: 0.0740 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 2s 33ms/sample - loss: 0.0021 - mae: 0.0311 - val_loss: 0.0156 - val_mae: 0.0736 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0023 - mae: 0.0315 - val_loss: 0.0148 - val_mae: 0.0714 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0018 - mae: 0.0282 - val_loss: 0.0144 - val_mae: 0.0713 - lr: 0.0010\n",
      "Epoch 19/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 0.0016 - mae: 0.0268\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0016 - mae: 0.0268 - val_loss: 0.0153 - val_mae: 0.0718 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0011 - mae: 0.0223 - val_loss: 0.0141 - val_mae: 0.0688 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 8.7345e-04 - mae: 0.0200 - val_loss: 0.0140 - val_mae: 0.0689 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 7.1724e-04 - mae: 0.0182 - val_loss: 0.0140 - val_mae: 0.0690 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 6.5588e-04 - mae: 0.0175 - val_loss: 0.0140 - val_mae: 0.0689 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 2s 33ms/sample - loss: 6.2862e-04 - mae: 0.0172 - val_loss: 0.0140 - val_mae: 0.0687 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 2s 33ms/sample - loss: 5.7115e-04 - mae: 0.0165 - val_loss: 0.0140 - val_mae: 0.0688 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 5.2610e-04 - mae: 0.0158 - val_loss: 0.0140 - val_mae: 0.0688 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 5.5324e-04 - mae: 0.0161 - val_loss: 0.0140 - val_mae: 0.0692 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 4.7446e-04 - mae: 0.0150\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 4.7446e-04 - mae: 0.0150 - val_loss: 0.0141 - val_mae: 0.0689 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 4.8135e-04 - mae: 0.0152 - val_loss: 0.0141 - val_mae: 0.0687 - lr: 1.0000e-05\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 4.9063e-04 - mae: 0.0153 - val_loss: 0.0140 - val_mae: 0.0688 - lr: 1.0000e-05\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 2s 33ms/sample - loss: 4.4055e-04 - mae: 0.0145 - val_loss: 0.0140 - val_mae: 0.0688 - lr: 1.0000e-05\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 5.0525e-04 - mae: 0.0156 - val_loss: 0.0140 - val_mae: 0.0688 - lr: 1.0000e-05\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 2s 33ms/sample - loss: 4.6951e-04 - mae: 0.0150 - val_loss: 0.0141 - val_mae: 0.0687 - lr: 1.0000e-05\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 4.4918e-04 - mae: 0.0146 - val_loss: 0.0140 - val_mae: 0.0687 - lr: 1.0000e-05\n",
      "Epoch 35/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 4.8189e-04 - mae: 0.0152\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "63/63 [==============================] - 2s 33ms/sample - loss: 4.8189e-04 - mae: 0.0152 - val_loss: 0.0140 - val_mae: 0.0688 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 4.2085e-04 - mae: 0.0142 - val_loss: 0.0140 - val_mae: 0.0688 - lr: 1.0000e-06\n",
      "1/1 [==============================] - 1s 933ms/step\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 8s 135ms/sample - loss: 0.0357 - mae: 0.1084 - val_loss: 0.0339 - val_mae: 0.1188 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0150 - mae: 0.0762 - val_loss: 0.0263 - val_mae: 0.1104 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 2s 39ms/sample - loss: 0.0111 - mae: 0.0676 - val_loss: 0.0184 - val_mae: 0.0833 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 2s 39ms/sample - loss: 0.0080 - mae: 0.0571 - val_loss: 0.0182 - val_mae: 0.0805 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 2s 39ms/sample - loss: 0.0062 - mae: 0.0515 - val_loss: 0.0164 - val_mae: 0.0777 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0056 - mae: 0.0496 - val_loss: 0.0156 - val_mae: 0.0751 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0044 - mae: 0.0429 - val_loss: 0.0158 - val_mae: 0.0772 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0042 - mae: 0.0434 - val_loss: 0.0158 - val_mae: 0.0742 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0033 - mae: 0.0380 - val_loss: 0.0151 - val_mae: 0.0729 - lr: 0.0010\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0030 - mae: 0.0365 - val_loss: 0.0158 - val_mae: 0.0802 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 2s 40ms/sample - loss: 0.0028 - mae: 0.0352 - val_loss: 0.0147 - val_mae: 0.0723 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 2s 39ms/sample - loss: 0.0025 - mae: 0.0333 - val_loss: 0.0143 - val_mae: 0.0708 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0021 - mae: 0.0307 - val_loss: 0.0148 - val_mae: 0.0748 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 2s 39ms/sample - loss: 0.0018 - mae: 0.0283 - val_loss: 0.0142 - val_mae: 0.0714 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0019 - mae: 0.0290 - val_loss: 0.0155 - val_mae: 0.0740 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0027 - mae: 0.0344 - val_loss: 0.0149 - val_mae: 0.0730 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 2s 39ms/sample - loss: 0.0022 - mae: 0.0315 - val_loss: 0.0142 - val_mae: 0.0700 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0020 - mae: 0.0300 - val_loss: 0.0145 - val_mae: 0.0713 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0019 - mae: 0.0288 - val_loss: 0.0143 - val_mae: 0.0694 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0015 - mae: 0.0258 - val_loss: 0.0148 - val_mae: 0.0705 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 2s 39ms/sample - loss: 0.0014 - mae: 0.0252 - val_loss: 0.0138 - val_mae: 0.0688 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0018 - mae: 0.0283 - val_loss: 0.0152 - val_mae: 0.0723 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0016 - mae: 0.0271 - val_loss: 0.0147 - val_mae: 0.0727 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0019 - mae: 0.0291 - val_loss: 0.0142 - val_mae: 0.0688 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0018 - mae: 0.0286 - val_loss: 0.0137 - val_mae: 0.0677 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0014 - mae: 0.0254 - val_loss: 0.0137 - val_mae: 0.0688 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 0.0014 - mae: 0.0254 - val_loss: 0.0153 - val_mae: 0.0767 - lr: 0.0010\n",
      "Epoch 28/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 0.0015 - mae: 0.0264\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "63/63 [==============================] - 2s 35ms/sample - loss: 0.0015 - mae: 0.0264 - val_loss: 0.0166 - val_mae: 0.0746 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 2s 39ms/sample - loss: 9.8916e-04 - mae: 0.0207 - val_loss: 0.0134 - val_mae: 0.0671 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 3s 40ms/sample - loss: 6.8579e-04 - mae: 0.0175 - val_loss: 0.0134 - val_mae: 0.0672 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 5.3785e-04 - mae: 0.0157 - val_loss: 0.0134 - val_mae: 0.0667 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 5.2174e-04 - mae: 0.0154 - val_loss: 0.0134 - val_mae: 0.0666 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 4.1868e-04 - mae: 0.0140 - val_loss: 0.0135 - val_mae: 0.0666 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 4.1079e-04 - mae: 0.0138 - val_loss: 0.0135 - val_mae: 0.0664 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 3.9620e-04 - mae: 0.0137 - val_loss: 0.0135 - val_mae: 0.0665 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 3.5042e-04 - mae: 0.0128\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 3.5042e-04 - mae: 0.0128 - val_loss: 0.0134 - val_mae: 0.0666 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 3.1065e-04 - mae: 0.0121 - val_loss: 0.0134 - val_mae: 0.0665 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 3.0455e-04 - mae: 0.0121 - val_loss: 0.0134 - val_mae: 0.0665 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 3.1244e-04 - mae: 0.0122 - val_loss: 0.0134 - val_mae: 0.0665 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 3.0939e-04 - mae: 0.0121 - val_loss: 0.0134 - val_mae: 0.0665 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 3.2817e-04 - mae: 0.0123 - val_loss: 0.0134 - val_mae: 0.0665 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 3.1823e-04 - mae: 0.0123 - val_loss: 0.0134 - val_mae: 0.0665 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 2.9980e-04 - mae: 0.0120\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 2.9980e-04 - mae: 0.0120 - val_loss: 0.0134 - val_mae: 0.0665 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 2.9047e-04 - mae: 0.0118 - val_loss: 0.0134 - val_mae: 0.0665 - lr: 1.0000e-06\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 2s 34ms/sample - loss: 3.1531e-04 - mae: 0.0122 - val_loss: 0.0134 - val_mae: 0.0665 - lr: 1.0000e-06\n",
      "1/1 [==============================] - 1s 970ms/step\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 11s 176ms/sample - loss: 0.0348 - mae: 0.1076 - val_loss: 0.0212 - val_mae: 0.0885 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 3s 42ms/sample - loss: 0.0164 - mae: 0.0806 - val_loss: 0.0181 - val_mae: 0.0811 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0112 - mae: 0.0681 - val_loss: 0.0182 - val_mae: 0.0871 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 3s 42ms/sample - loss: 0.0085 - mae: 0.0601 - val_loss: 0.0169 - val_mae: 0.0818 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0073 - mae: 0.0557 - val_loss: 0.0178 - val_mae: 0.0823 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0071 - mae: 0.0558 - val_loss: 0.0177 - val_mae: 0.0781 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 3s 42ms/sample - loss: 0.0056 - mae: 0.0493 - val_loss: 0.0158 - val_mae: 0.0763 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 3s 41ms/sample - loss: 0.0051 - mae: 0.0465 - val_loss: 0.0146 - val_mae: 0.0712 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0037 - mae: 0.0401 - val_loss: 0.0162 - val_mae: 0.0765 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 3s 42ms/sample - loss: 0.0031 - mae: 0.0366 - val_loss: 0.0145 - val_mae: 0.0710 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0028 - mae: 0.0350 - val_loss: 0.0154 - val_mae: 0.0748 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 3s 42ms/sample - loss: 0.0024 - mae: 0.0330 - val_loss: 0.0145 - val_mae: 0.0704 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 3s 42ms/sample - loss: 0.0022 - mae: 0.0308 - val_loss: 0.0145 - val_mae: 0.0698 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 3s 41ms/sample - loss: 0.0023 - mae: 0.0317 - val_loss: 0.0143 - val_mae: 0.0706 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0021 - mae: 0.0302 - val_loss: 0.0144 - val_mae: 0.0700 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0019 - mae: 0.0298 - val_loss: 0.0144 - val_mae: 0.0697 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0016 - mae: 0.0269 - val_loss: 0.0156 - val_mae: 0.0718 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 3s 41ms/sample - loss: 0.0016 - mae: 0.0272 - val_loss: 0.0141 - val_mae: 0.0688 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0014 - mae: 0.0253 - val_loss: 0.0143 - val_mae: 0.0704 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 3s 42ms/sample - loss: 0.0014 - mae: 0.0256 - val_loss: 0.0139 - val_mae: 0.0684 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0018 - mae: 0.0290 - val_loss: 0.0155 - val_mae: 0.0727 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0017 - mae: 0.0278 - val_loss: 0.0149 - val_mae: 0.0706 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0017 - mae: 0.0274 - val_loss: 0.0142 - val_mae: 0.0704 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0013 - mae: 0.0242 - val_loss: 0.0143 - val_mae: 0.0691 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0011 - mae: 0.0221 - val_loss: 0.0142 - val_mae: 0.0688 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0013 - mae: 0.0242 - val_loss: 0.0140 - val_mae: 0.0679 - lr: 0.0010\n",
      "Epoch 27/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 0.0010 - mae: 0.0216\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0010 - mae: 0.0216 - val_loss: 0.0148 - val_mae: 0.0695 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 3s 42ms/sample - loss: 7.6780e-04 - mae: 0.0186 - val_loss: 0.0139 - val_mae: 0.0673 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 5.5469e-04 - mae: 0.0159 - val_loss: 0.0139 - val_mae: 0.0673 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 5.4408e-04 - mae: 0.0158 - val_loss: 0.0139 - val_mae: 0.0671 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 4.3993e-04 - mae: 0.0143 - val_loss: 0.0139 - val_mae: 0.0671 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 4.3315e-04 - mae: 0.0141 - val_loss: 0.0141 - val_mae: 0.0673 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 4.1024e-04 - mae: 0.0138 - val_loss: 0.0142 - val_mae: 0.0675 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 3.9721e-04 - mae: 0.0136\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "63/63 [==============================] - 3s 42ms/sample - loss: 3.9721e-04 - mae: 0.0136 - val_loss: 0.0138 - val_mae: 0.0682 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 3s 41ms/sample - loss: 4.0995e-04 - mae: 0.0137 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 3s 41ms/sample - loss: 3.8448e-04 - mae: 0.0134 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.2727e-04 - mae: 0.0124 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.6019e-04 - mae: 0.0129 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.5688e-04 - mae: 0.0129 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 3s 42ms/sample - loss: 3.5902e-04 - mae: 0.0129 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 3s 43ms/sample - loss: 3.6770e-04 - mae: 0.0131 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.4203e-04 - mae: 0.0126 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-05\n",
      "Epoch 43/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 3.4044e-04 - mae: 0.0125\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.4044e-04 - mae: 0.0125 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-05\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 3.5164e-04 - mae: 0.0127 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-06\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 3.6009e-04 - mae: 0.0129 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-06\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.5236e-04 - mae: 0.0128 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-06\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 3.5840e-04 - mae: 0.0129 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-06\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.3879e-04 - mae: 0.0126 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-06\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.5315e-04 - mae: 0.0128 - val_loss: 0.0138 - val_mae: 0.0669 - lr: 1.0000e-06\n",
      "Epoch 50/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 3.5929e-04 - mae: 0.0130\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.5929e-04 - mae: 0.0130 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-06\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.5645e-04 - mae: 0.0128 - val_loss: 0.0138 - val_mae: 0.0669 - lr: 1.0000e-07\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.2483e-04 - mae: 0.0123 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-07\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.2789e-04 - mae: 0.0123 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-07\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.3451e-04 - mae: 0.0124 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-07\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.1944e-04 - mae: 0.0122 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-07\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.3956e-04 - mae: 0.0126 - val_loss: 0.0138 - val_mae: 0.0670 - lr: 1.0000e-07\n",
      "1/1 [==============================] - 1s 1s/step\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 11s 178ms/sample - loss: 0.0375 - mae: 0.1111 - val_loss: 0.0308 - val_mae: 0.1152 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 3s 41ms/sample - loss: 0.0154 - mae: 0.0781 - val_loss: 0.0178 - val_mae: 0.0810 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 3s 43ms/sample - loss: 0.0118 - mae: 0.0689 - val_loss: 0.0176 - val_mae: 0.0803 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0094 - mae: 0.0629 - val_loss: 0.0227 - val_mae: 0.0985 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 3s 43ms/sample - loss: 0.0087 - mae: 0.0607 - val_loss: 0.0176 - val_mae: 0.0825 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 3s 43ms/sample - loss: 0.0067 - mae: 0.0535 - val_loss: 0.0158 - val_mae: 0.0783 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 3s 41ms/sample - loss: 0.0051 - mae: 0.0475 - val_loss: 0.0158 - val_mae: 0.0747 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0047 - mae: 0.0463 - val_loss: 0.0180 - val_mae: 0.0823 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 3s 42ms/sample - loss: 0.0035 - mae: 0.0393 - val_loss: 0.0150 - val_mae: 0.0721 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 3s 43ms/sample - loss: 0.0029 - mae: 0.0357 - val_loss: 0.0143 - val_mae: 0.0716 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0027 - mae: 0.0351 - val_loss: 0.0155 - val_mae: 0.0737 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0030 - mae: 0.0368 - val_loss: 0.0147 - val_mae: 0.0722 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0032 - mae: 0.0366 - val_loss: 0.0170 - val_mae: 0.0793 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 3s 42ms/sample - loss: 0.0043 - mae: 0.0436 - val_loss: 0.0141 - val_mae: 0.0715 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0029 - mae: 0.0365 - val_loss: 0.0147 - val_mae: 0.0743 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0029 - mae: 0.0367 - val_loss: 0.0150 - val_mae: 0.0751 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0025 - mae: 0.0343 - val_loss: 0.0160 - val_mae: 0.0742 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0020 - mae: 0.0301 - val_loss: 0.0149 - val_mae: 0.0711 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 3s 42ms/sample - loss: 0.0018 - mae: 0.0287 - val_loss: 0.0138 - val_mae: 0.0688 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0015 - mae: 0.0259 - val_loss: 0.0139 - val_mae: 0.0701 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0017 - mae: 0.0284 - val_loss: 0.0139 - val_mae: 0.0689 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0017 - mae: 0.0277 - val_loss: 0.0146 - val_mae: 0.0696 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 3s 42ms/sample - loss: 0.0018 - mae: 0.0283 - val_loss: 0.0138 - val_mae: 0.0690 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0013 - mae: 0.0245 - val_loss: 0.0141 - val_mae: 0.0687 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 0.0013 - mae: 0.0239 - val_loss: 0.0142 - val_mae: 0.0692 - lr: 0.0010\n",
      "Epoch 26/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 0.0012 - mae: 0.0231\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 0.0012 - mae: 0.0231 - val_loss: 0.0145 - val_mae: 0.0727 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 3s 42ms/sample - loss: 0.0011 - mae: 0.0225 - val_loss: 0.0135 - val_mae: 0.0677 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 6.9357e-04 - mae: 0.0181 - val_loss: 0.0136 - val_mae: 0.0673 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 6.0003e-04 - mae: 0.0168 - val_loss: 0.0135 - val_mae: 0.0678 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 3s 42ms/sample - loss: 5.4837e-04 - mae: 0.0162 - val_loss: 0.0135 - val_mae: 0.0674 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 4.5646e-04 - mae: 0.0147 - val_loss: 0.0135 - val_mae: 0.0672 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 4.7897e-04 - mae: 0.0151 - val_loss: 0.0135 - val_mae: 0.0678 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 4.6203e-04 - mae: 0.0148 - val_loss: 0.0136 - val_mae: 0.0670 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 3.9568e-04 - mae: 0.0137\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 3.9568e-04 - mae: 0.0137 - val_loss: 0.0136 - val_mae: 0.0671 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 3s 42ms/sample - loss: 3.6882e-04 - mae: 0.0133 - val_loss: 0.0135 - val_mae: 0.0671 - lr: 1.0000e-05\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 3.8876e-04 - mae: 0.0136 - val_loss: 0.0135 - val_mae: 0.0671 - lr: 1.0000e-05\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 3.6587e-04 - mae: 0.0132 - val_loss: 0.0135 - val_mae: 0.0670 - lr: 1.0000e-05\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 3s 42ms/sample - loss: 4.0828e-04 - mae: 0.0139 - val_loss: 0.0135 - val_mae: 0.0671 - lr: 1.0000e-05\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 3.6732e-04 - mae: 0.0132 - val_loss: 0.0135 - val_mae: 0.0671 - lr: 1.0000e-05\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 3s 43ms/sample - loss: 3.9459e-04 - mae: 0.0137 - val_loss: 0.0135 - val_mae: 0.0671 - lr: 1.0000e-05\n",
      "Epoch 41/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 3.9886e-04 - mae: 0.0138\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.9886e-04 - mae: 0.0138 - val_loss: 0.0135 - val_mae: 0.0670 - lr: 1.0000e-05\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 3.7716e-04 - mae: 0.0134 - val_loss: 0.0135 - val_mae: 0.0670 - lr: 1.0000e-06\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 3.8288e-04 - mae: 0.0135 - val_loss: 0.0135 - val_mae: 0.0670 - lr: 1.0000e-06\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.5432e-04 - mae: 0.0130 - val_loss: 0.0135 - val_mae: 0.0670 - lr: 1.0000e-06\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 3.7079e-04 - mae: 0.0133 - val_loss: 0.0135 - val_mae: 0.0670 - lr: 1.0000e-06\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.6439e-04 - mae: 0.0132 - val_loss: 0.0135 - val_mae: 0.0670 - lr: 1.0000e-06\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.5448e-04 - mae: 0.0130 - val_loss: 0.0135 - val_mae: 0.0670 - lr: 1.0000e-06\n",
      "Epoch 48/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 3.5805e-04 - mae: 0.0130\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.5805e-04 - mae: 0.0130 - val_loss: 0.0135 - val_mae: 0.0670 - lr: 1.0000e-06\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.7018e-04 - mae: 0.0132 - val_loss: 0.0135 - val_mae: 0.0670 - lr: 1.0000e-07\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 2s 37ms/sample - loss: 3.3495e-04 - mae: 0.0126 - val_loss: 0.0135 - val_mae: 0.0670 - lr: 1.0000e-07\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 3.9575e-04 - mae: 0.0136 - val_loss: 0.0135 - val_mae: 0.0670 - lr: 1.0000e-07\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 3.7927e-04 - mae: 0.0134 - val_loss: 0.0135 - val_mae: 0.0670 - lr: 1.0000e-07\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 3.7348e-04 - mae: 0.0133 - val_loss: 0.0135 - val_mae: 0.0670 - lr: 1.0000e-07\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 3.6812e-04 - mae: 0.0132 - val_loss: 0.0135 - val_mae: 0.0670 - lr: 1.0000e-07\n",
      "Epoch 55/100\n",
      " 0/63 [..............................] - ETA: 0s - loss: 3.6061e-04 - mae: 0.0131\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "63/63 [==============================] - 2s 38ms/sample - loss: 3.6061e-04 - mae: 0.0131 - val_loss: 0.0135 - val_mae: 0.0670 - lr: 1.0000e-07\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "res = opt_conf(data, tune=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4181008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3_5_7_7_1_3 4.22\n",
      "2 3_5_7_7_2_3 4.24\n",
      "3 3_5_7_5_2_1 4.27\n",
      "4 3_5_7_5_1_1 4.27\n",
      "5 3_5_7_7_2_1 4.28\n",
      "6 3_5_7_3_1_3 4.3\n",
      "7 3_5_7_3_1_1 4.32\n",
      "8 3_5_7_7_1_1 4.33\n",
      "9 3_5_7_5_1_3 4.35\n",
      "10 3_5_7_3_2_3 4.36\n",
      "11 3_5_7_5_2_3 4.36\n",
      "12 3_5_7_3_2_1 4.36\n"
     ]
    }
   ],
   "source": [
    "# kern, layers, lstm, dense\n",
    "for pos, (k, v) in enumerate(sorted(res.items(), key=lambda item: item[1])):\n",
    "    print(pos+1, k, round(v, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0687c04",
   "metadata": {},
   "source": [
    "## Best Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e060519",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conf = model_config = {\n",
    "    'kernel_sizes': (3, 5, 7),\n",
    "    'n_layers': 7,\n",
    "    'lstm': 1,\n",
    "    'dense': 3,\n",
    "    'activation': 'tanh',\n",
    "    'norm': True,\n",
    "    'reshape_out': False,\n",
    "    'epochs': 100\n",
    "}\n",
    "mod = get_mod(conf=model_conf, version='3_5_7_7_1_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cc953f",
   "metadata": {},
   "source": [
    "# PDB set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c8a096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_pickle(f'{DATA_DIR}/pdb_measure.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8ed8519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_pickle(os.path.join(DATA_DIR, 'pdb_measure.p'))\n",
    "test_data = test_data[~test_data.mutant]\n",
    "test_data.rename(columns={'seq1':'n_seq', 'seq2':'c_seq', 'n_list':'n_crick_mut', 'c_list':'c_crick_mut'}, inplace=True)\n",
    "test_data['n_seq'] = test_data['n_seq'].apply(lambda x: x[1:-1])\n",
    "test_data['c_seq'] = test_data['c_seq'].apply(lambda x: x[1:-1])\n",
    "test_data['train_seq'] = test_data.apply(lambda x: x['n_seq'] + x['c_seq'], axis=1)\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "daa9ee05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "seq = []\n",
    "tr = []\n",
    "for n,r in test_data.iterrows():\n",
    "    seq.append(r['n_seq'] + r['c_seq'])\n",
    "    tr.append(np.mean((r['rot'][0::2] + r['rot'][1::2]) / 2)/2)\n",
    "res = mod.predict(seq)\n",
    "pr = []\n",
    "for n,r in res.iterrows():\n",
    "    pr.append(np.mean(r['N_pred'])/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e0039a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'HAMPpred model #1')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "title_font_size = 12\n",
    "label_font_size = 10\n",
    "tick_font_size = 8\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(4, 4), dpi=200)\n",
    "\n",
    "colors = ['white' if i=='1_2l7i.pdb' else 'grey' for i in test_data.index]\n",
    "\n",
    "ax.scatter(pr, tr, color=colors, lw=0.75, edgecolors='k', s=50)\n",
    "\n",
    "ax.set_ylim(-26, 26)\n",
    "ax.set_xlim(-26, 26)\n",
    "ax.plot([0, 1], [0, 1], '--', transform=ax.transAxes, color='grey')\n",
    "\n",
    "ax.set_xlabel(r'Predicted rotation [$\\degree$]', size=label_font_size)\n",
    "ax.set_ylabel(r'Experimental structure rotation [$\\degree$]', size=label_font_size)\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=tick_font_size)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=tick_font_size)\n",
    "ax.set_title(\"HAMPpred model #1\", size=title_font_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "161ec50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.297805398805629"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((np.asarray(tr)-np.asarray(pr)) **2 ) ** 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
