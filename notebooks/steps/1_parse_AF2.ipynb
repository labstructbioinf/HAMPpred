{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2caf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import multiprocessing\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from Bio import SeqIO\n",
    "from glob import glob\n",
    "from functools import partial\n",
    "sys.path.append('../../')\n",
    "from hamp_pred.external.lbs.sequence import mmseqs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1026f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data/input'\n",
    "out_path = os.path.join(data_dir, 'af2_full')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4868a7e3",
   "metadata": {},
   "source": [
    "# Get HAMP sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1e966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and cluster sequences from https://pubmed.ncbi.nlm.nih.gov/20184894/\n",
    "msa = list(SeqIO.parse(os.path.join(data_dir, 'hamp_msa.fasta'), 'fasta'))\n",
    "\n",
    "msa_df = pd.DataFrame(\n",
    "        [(str(i.seq).replace('-', ''), i.id) for i in msa],\n",
    "    columns=['sequence', 'id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "910bfa4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6456"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(msa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f9cddcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createdb /tmp/_ectbcew tmp/17013954945929323354/input --max-seq-len 65535 --dont-split-seq-by-len 1 --dbtype 0 --dont-shuffle 1 --id-offset 0 --compressed 0 -v 3 \n",
      "\n",
      "Converting sequences\n",
      "[\n",
      "Time for merging files: 0h 0m 0s 72ms\n",
      "Time for merging files: 0h 0m 0s 82ms\n",
      "Time for merging files: 0h 0m 0s 0ms\n",
      "Time for processing: 0h 0m 0s 360ms\n",
      "kmermatcher tmp/17013954945929323354/input tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/pref --sub-mat blosum62.out --alph-size 13 --min-seq-id 0.7 --kmer-per-seq 21 --adjust-kmer-len 0 --mask 0 --mask-lower-case 0 --cov-mode 0 -k 0 -c 0.7 --max-seq-len 65535 --hash-shift 5 --split-memory-limit 0 --include-only-extendable 0 --skip-n-repeat-kmer 0 --threads 20 --compressed 0 -v 3 \n",
      "\n",
      "Database size: 6456 type: Aminoacid\n",
      "Reduced amino acid alphabet: (A S T) (C) (D B N) (E Q Z) (F Y) (G) (H) (I V) (K R) (L J M) (P) (W) (X) \n",
      "\n",
      "Estimated memory consumption 2 MB\n",
      "Generate k-mers list for 1 split\n",
      "[=================================================================] 6.46K 0s 4ms\n",
      "Sort kmer 0h 0m 0s 42ms\n",
      "Sort by rep. sequence 0h 0m 0s 63ms\n",
      "Time for fill: 0h 0m 0s 1ms\n",
      "Time for merging files: 0h 0m 0s 65ms\n",
      "Time for processing: 0h 0m 0s 260ms\n",
      "rescorediagonal tmp/17013954945929323354/input tmp/17013954945929323354/input tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/pref tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/pref_rescore1 --sub-mat blosum62.out --rescore-mode 0 --filter-hits 0 -e 0.001 -c 0.7 -a 0 --cov-mode 0 --min-seq-id 0.7 --min-aln-len 0 --seq-id-mode 0 --add-self-matches 0 --sort-results 0 --global-alignment 0 --db-load-mode 0 --threads 20 --compressed 0 -v 3 \n",
      "\n",
      "[=================================================================] 6.46K 0s 13ms\n",
      "Time for merging files: 0h 0m 0s 59ms\n",
      "Time for processing: 0h 0m 0s 192ms\n",
      "clust tmp/17013954945929323354/input tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/pref_rescore1 tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/pre_clust --cluster-mode 0 --max-iterations 1000 --similarity-type 2 --threads 20 --compressed 0 -v 3 \n",
      "\n",
      "Clustering mode: Set Cover\n",
      "[=================================================================] 6.46K 0s 2ms\n",
      "Sort entries\n",
      "Find missing connections\n",
      "Found 1762 new connections.\n",
      "Reconstruct initial order\n",
      "[=================================================================] 6.46K 0s 3ms\n",
      "Add missing connections\n",
      "[=================================================================] 6.46K 0s 0ms\n",
      "\n",
      "Time for read in: 0h 0m 0s 50ms\n",
      "Total time: 0h 0m 0s 80ms\n",
      "\n",
      "Size of the sequence database: 6456\n",
      "Size of the alignment database: 6456\n",
      "Number of clusters: 5614\n",
      "\n",
      "Writing results 0h 0m 0s 1ms\n",
      "Time for merging files: 0h 0m 0s 60ms\n",
      "Time for processing: 0h 0m 0s 182ms\n",
      "createsubdb tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/order_redundancy tmp/17013954945929323354/input tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/input_step_redundancy \n",
      "\n",
      "Time for merging files: 0h 0m 0s 0ms\n",
      "Time for processing: 0h 0m 0s 34ms\n",
      "createsubdb tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/order_redundancy tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/pref tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/pref_filter1 \n",
      "\n",
      "Time for merging files: 0h 0m 0s 14ms\n",
      "Time for processing: 0h 0m 0s 27ms\n",
      "filterdb tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/pref_filter1 tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/pref_filter2 --filter-file tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/order_redundancy \n",
      "\n",
      "Filtering with a filter files.\n",
      "[=================================================================] 5.61K 0s 0ms\n",
      "Time for merging files: 0h 0m 0s 58ms\n",
      "Time for processing: 0h 0m 0s 113ms\n",
      "rescorediagonal tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/input_step_redundancy tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/input_step_redundancy tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/pref_filter2 tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/pref_rescore2 --sub-mat blosum62.out --rescore-mode 1 --filter-hits 1 -e 0.001 -c 0.7 -a 0 --cov-mode 0 --min-seq-id 0.7 --min-aln-len 0 --seq-id-mode 0 --add-self-matches 0 --sort-results 0 --global-alignment 0 --db-load-mode 0 --threads 20 --compressed 0 -v 3 \n",
      "\n",
      "[=================================================================] 5.61K 0s 24ms\n",
      "Time for merging files: 0h 0m 0s 76ms\n",
      "Time for processing: 0h 0m 0s 148ms\n",
      "align tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/input_step_redundancy tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/input_step_redundancy tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/pref_rescore2 tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/aln --sub-mat blosum62.out -a 0 --alignment-mode 3 -e 0.001 --min-seq-id 0.7 --min-aln-len 0 --seq-id-mode 0 --alt-ali 0 -c 0.7 --cov-mode 0 --max-seq-len 65535 --comp-bias-corr 1 --realign 0 --max-rejected 2147483647 --max-accept 2147483647 --add-self-matches 0 --db-load-mode 0 --pca 1 --pcb 1.5 --score-bias 0 --gap-open 11 --gap-extend 1 --threads 20 --compressed 0 -v 3 \n",
      "\n",
      "Compute score, coverage and sequence identity\n",
      "Query database size: 5614 type: Aminoacid\n",
      "Target database size: 5614 type: Aminoacid\n",
      "Calculation of alignments\n",
      "[=================================================================] 5.61K 0s 37ms\n",
      "Time for merging files: 0h 0m 0s 59ms\n",
      "\n",
      "9778 alignments calculated.\n",
      "5667 sequence pairs passed the thresholds (0.579566 of overall calculated).\n",
      "1.009441 hits per query sequence.\n",
      "Time for processing: 0h 0m 0s 206ms\n",
      "clust tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/input_step_redundancy tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/aln tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/clust --cluster-mode 0 --max-iterations 1000 --similarity-type 2 --threads 20 --compressed 0 -v 3 \n",
      "\n",
      "Clustering mode: Set Cover\n",
      "[=================================================================] 5.61K 0s 3ms\n",
      "Sort entries\n",
      "Find missing connections\n",
      "Found 53 new connections.\n",
      "Reconstruct initial order\n",
      "[=================================================================] 5.61K 0s 1ms\n",
      "Add missing connections\n",
      "[=================================================================] 5.61K 0s 1ms\n",
      "\n",
      "Time for read in: 0h 0m 0s 45ms\n",
      "Total time: 0h 0m 0s 90ms\n",
      "\n",
      "Size of the sequence database: 5614\n",
      "Size of the alignment database: 5614\n",
      "Number of clusters: 5561\n",
      "\n",
      "Writing results 0h 0m 0s 1ms\n",
      "Time for merging files: 0h 0m 0s 55ms\n",
      "Time for processing: 0h 0m 0s 215ms\n",
      "mergeclusters tmp/17013954945929323354/input tmp/17013954945929323354/clu_tmp/17242563648264802310/clu_redundancy tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/pre_clust tmp/17013954945929323354/clu_tmp/17242563648264802310/linclust/5647431470958174557/clust --threads 20 --compressed 0 -v 3 \n",
      "\n",
      "List amount 6456\n",
      "Clustering step 1\n",
      "Clustering step 2\n",
      "Writing the results\n",
      "Time for merging files: 0h 0m 0s 65ms\n",
      "Time for processing: 0h 0m 0s 188ms\n",
      "createsubdb tmp/17013954945929323354/clu_tmp/17242563648264802310/clu_redundancy tmp/17013954945929323354/input tmp/17013954945929323354/clu_tmp/17242563648264802310/input_step_redundancy \n",
      "\n",
      "Time for merging files: 0h 0m 0s 13ms\n",
      "Time for processing: 0h 0m 0s 27ms\n",
      "prefilter tmp/17013954945929323354/clu_tmp/17242563648264802310/input_step_redundancy tmp/17013954945929323354/clu_tmp/17242563648264802310/input_step_redundancy tmp/17013954945929323354/clu_tmp/17242563648264802310/pref_step0 --sub-mat blosum62.out --seed-sub-mat VTML80.out -s 1 -k 0 --k-score 2147483647 --alph-size 21 --max-seq-len 65535 --max-seqs 20 --split 0 --split-mode 2 --split-memory-limit 0 -c 0.7 --cov-mode 0 --comp-bias-corr 0 --diag-score 0 --exact-kmer-matching 0 --mask 1 --mask-lower-case 0 --min-ungapped-score 0 --add-self-matches 0 --spaced-kmer-mode 1 --db-load-mode 0 --pca 1 --pcb 1.5 --threads 20 --compressed 0 -v 3 \n",
      "\n",
      "Using 20 threads.\n",
      "Estimated memory consumption 984 MB\n",
      "Target database size: 5561 type: Aminoacid\n",
      "Query database size: 5561 type: Aminoacid\n",
      "Process prefiltering step 1 of 1\n",
      "\n",
      "Index table k-mer threshold: 154 at k-mer size 6 \n",
      "Index table: counting k-mers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================================================] 5.56K 0s 46ms\n",
      "Index table: Masked residues: 75\n",
      "Index table: fill\n",
      "[=================================================================] 5.56K 0s 4ms\n",
      "Index statistics\n",
      "Entries:          83252\n",
      "DB size:          488 MB\n",
      "Avg k-mer size:   0.001301\n",
      "Top 10 k-mers\n",
      "    DEGLTF\t82\n",
      "    EIQAFN\t62\n",
      "    DEGLNF\t49\n",
      "    EIDAFN\t35\n",
      "    RREGAR\t32\n",
      "    EIQSFN\t21\n",
      "    PVSDGQ\t20\n",
      "    IGMREV\t19\n",
      "    DEGMTV\t16\n",
      "    PVSDGR\t14\n",
      "Time for index table init: 0h 0m 0s 807ms\n",
      "k-mer similarity threshold: 154\n",
      "\tk-mers per position = 0.475694, k-mer match probability: 0.000000\n",
      "k-mer match probability: 0.000000\n",
      "\n",
      "Starting prefiltering scores calculation (step 1 of 1)\n",
      "Query db start  1 to 5561\n",
      "Target db start  1 to 5561\n",
      "[=================================================================] 5.56K 0s 10ms\n",
      "\n",
      "0.473296 k-mers per position\n",
      "27 DB matches per sequence\n",
      "0 overflows\n",
      "6 sequences passed prefiltering per query sequence\n",
      "3 median result list length\n",
      "0 sequences with 0 size result lists\n",
      "\n",
      "Time for prefiltering scores calculation: 0h 0m 0s 22ms\n",
      "Time for merging files: 0h 0m 0s 77ms\n",
      "Time for processing: 0h 0m 1s 645ms\n",
      "align tmp/17013954945929323354/clu_tmp/17242563648264802310/input_step_redundancy tmp/17013954945929323354/clu_tmp/17242563648264802310/input_step_redundancy tmp/17013954945929323354/clu_tmp/17242563648264802310/pref_step0 tmp/17013954945929323354/clu_tmp/17242563648264802310/aln_step0 --sub-mat blosum62.out -a 0 --alignment-mode 3 -e 0.001 --min-seq-id 0.7 --min-aln-len 0 --seq-id-mode 0 --alt-ali 0 -c 0.7 --cov-mode 0 --max-seq-len 65535 --comp-bias-corr 0 --realign 0 --max-rejected 2147483647 --max-accept 2147483647 --add-self-matches 0 --db-load-mode 0 --pca 1 --pcb 1.5 --score-bias 0 --gap-open 11 --gap-extend 1 --threads 20 --compressed 0 -v 3 \n",
      "\n",
      "Compute score, coverage and sequence identity\n",
      "Query database size: 5561 type: Aminoacid\n",
      "Target database size: 5561 type: Aminoacid\n",
      "Calculation of alignments\n",
      "[=================================================================] 5.56K 0s 98ms\n",
      "Time for merging files: 0h 0m 0s 15ms\n",
      "\n",
      "33714 alignments calculated.\n",
      "5948 sequence pairs passed the thresholds (0.176425 of overall calculated).\n",
      "1.069592 hits per query sequence.\n",
      "Time for processing: 0h 0m 0s 222ms\n",
      "clust tmp/17013954945929323354/clu_tmp/17242563648264802310/input_step_redundancy tmp/17013954945929323354/clu_tmp/17242563648264802310/aln_step0 tmp/17013954945929323354/clu_tmp/17242563648264802310/clu_step0 --cluster-mode 0 --max-iterations 1000 --similarity-type 2 --threads 20 --compressed 0 -v 3 \n",
      "\n",
      "Clustering mode: Set Cover\n",
      "[=================================================================] 5.56K 0s 10ms\n",
      "Sort entries\n",
      "Find missing connections\n",
      "Found 3 new connections.\n",
      "Reconstruct initial order\n",
      "[=================================================================] 5.56K 0s 1ms\n",
      "Add missing connections\n",
      "[=================================================================] 5.56K 0s 0ms\n",
      "\n",
      "Time for read in: 0h 0m 0s 64ms\n",
      "Total time: 0h 0m 0s 96ms\n",
      "\n",
      "Size of the sequence database: 5561\n",
      "Size of the alignment database: 5561\n",
      "Number of clusters: 5388\n",
      "\n",
      "Writing results 0h 0m 0s 1ms\n",
      "Time for merging files: 0h 0m 0s 56ms\n",
      "Time for processing: 0h 0m 0s 185ms\n",
      "mergeclusters tmp/17013954945929323354/input tmp/17013954945929323354/clu tmp/17013954945929323354/clu_tmp/17242563648264802310/clu_redundancy tmp/17013954945929323354/clu_tmp/17242563648264802310/clu_step0 --threads 20 --compressed 0 -v 3 \n",
      "\n",
      "List amount 6456\n",
      "Clustering step 1\n",
      "Clustering step 2\n",
      "Writing the results\n",
      "Time for merging files: 0h 0m 0s 367ms\n",
      "Time for processing: 0h 0m 0s 471ms\n",
      "createtsv tmp/17013954945929323354/input tmp/17013954945929323354/input tmp/17013954945929323354/clu tmp/17013954945929323354/cluster.tsv --threads 20 -v 3 \n",
      "\n",
      "Time for merging files: 0h 0m 0s 73ms\n",
      "Time for processing: 0h 0m 0s 201ms\n",
      "result2repseq tmp/17013954945929323354/input tmp/17013954945929323354/clu tmp/17013954945929323354/clu_rep --threads 20 -v 3 \n",
      "\n",
      "Start computing representative sequences.\n",
      "[=================================================================] 5.39K 0s 9ms\n",
      "\n",
      "Time for merging files: 0h 0m 0s 61ms\n",
      "Time for processing: 0h 0m 0s 156ms\n",
      "result2flat tmp/17013954945929323354/input tmp/17013954945929323354/input tmp/17013954945929323354/clu_rep tmp/17013954945929323354/rep_seq.fasta --use-fasta-header -v 3 \n",
      "\n",
      "Time for processing: 0h 0m 0s 124ms\n",
      "createseqfiledb tmp/17013954945929323354/input tmp/17013954945929323354/clu tmp/17013954945929323354/clu_seqs --threads 20 -v 3 \n",
      "\n",
      "Time for merging files: 0h 0m 0s 87ms\n",
      "Time for processing: 0h 0m 0s 242ms\n",
      "result2flat tmp/17013954945929323354/input tmp/17013954945929323354/input tmp/17013954945929323354/clu_seqs tmp/17013954945929323354/all_seqs.fasta -v 3 \n",
      "\n",
      "Time for processing: 0h 0m 0s 77ms\n"
     ]
    }
   ],
   "source": [
    "# remove redundancy at 70% ident and 70% coverage\n",
    "cl = mmseqs2.MMSeqsClusterer()\n",
    "msa_df_clustered = cl.cluster(msa_df, min_identity=0.7, coverage=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b101d9cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5388"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msa_df_clustered = msa_df_clustered.groupby('clust_id').head(1)\n",
    "len(msa_df_clustered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29cbe631",
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_df_clustered.to_pickle(os.path.join(data_dir, 'hamp_master.p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169e00f4",
   "metadata": {},
   "source": [
    "# Unpack AF2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a464b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_models(models_dir, out_path, df_clustered):\n",
    "    for out_file in glob(os.path.join(models_dir, '*', '*result.zip')):\n",
    "\n",
    "        #print(out_file)\n",
    "        \n",
    "        seq_id = int(out_file.split('/')[-1].replace('.result.zip', '').split(\"_\")[1])\n",
    "        assert seq_id in df_clustered.index\n",
    "\n",
    "        tmp = os.path.join(out_path, str(seq_id))\n",
    "        \n",
    "        if not os.path.exists(tmp):\n",
    "        \n",
    "            os.system(f'mkdir -p {tmp}')\n",
    "\n",
    "            # -n ensures that existing files are not overwritten\n",
    "            os.system(f'unzip -n -j \"{out_file}\" \"*relaxed*\" -d {tmp}')\n",
    "        \n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02500a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# af2 model files are avaliable upon request \n",
    "#\n",
    "df_clustered = pd.read_pickle(os.path.join(data_dir, 'hamp_master.p'))\n",
    "\n",
    "# get aligned sequences\n",
    "msa = list(SeqIO.parse(os.path.join(data_dir, 'hamp_msa.fasta'), 'fasta'))\n",
    "hampid2alnseq = pd.DataFrame(\n",
    "        [(str(i.seq), i.id) for i in msa],\n",
    "    columns=['sequence', 'id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f88946da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5388, 6456, 6456)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_clustered), len(msa), len(hampid2alnseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "568cb18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip af2 models bundles\n",
    "# uncomment me\n",
    "models_dir = '/home/nfs/jludwiczak/calc/hamp_olek/hamp_final/out'\n",
    "\n",
    "extract_models(models_dir, out_path, df_clustered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd02717",
   "metadata": {},
   "source": [
    "# Analyses AF2 models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adcecd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Measure with SamCC\n",
    "### Get AF2 scores\n",
    "### Store PDB file link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14b394d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To measure af2 hamp models use samCC turbo https://academic.oup.com/bioinformatics/article/36/22-23/5368/6039120\n",
    "sys.path.append('../../hamp_pred/')\n",
    "from utils.measure import measure_one_HAMP, get_ref_crick\n",
    "from utils.tools import diffangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bfbdb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiprocess(func, tasks, n_cores, tasks_per_core=1):  \n",
    "    stdout_queue = multiprocessing.Queue()\n",
    "    pool = multiprocessing.Pool(processes=n_cores, initargs=[stdout_queue], maxtasksperchild=tasks_per_core)\n",
    "    for i, data in enumerate(pool.map(func, tasks), 1):\n",
    "        yield data\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f06a6d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# referece Crick angles\n",
    "crangles = {'a':19.5,'b':122.35,'c':-134.78,'d': -31.92,'e':70.92 ,'f':173.78,'g':-83.35}\n",
    "\n",
    "# aa names mapping\n",
    "AA_3_to_1 = {'CYS': 'C', 'ASP': 'D', 'SER': 'S', 'GLN': 'Q', 'LYS': 'K',\n",
    "             'ILE': 'I', 'PRO': 'P', 'THR': 'T', 'PHE': 'F', 'ASN': 'N', \n",
    "             'GLY': 'G', 'HIS': 'H', 'LEU': 'L', 'ARG': 'R', 'TRP': 'W', \n",
    "             'ALA': 'A', 'VAL':'V', 'GLU': 'E', 'TYR': 'Y', 'MET': 'M'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ff23878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ranges for helix 1 and helix 2 in MSA\n",
    "h1_msa_start = 5 #4\n",
    "h1_msa_stop = 18\n",
    "\n",
    "h2_msa_start = 85 # 84\n",
    "h2_msa_stop = 98\n",
    "\n",
    "start_hep = 'a' #g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12a08507",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fadb98c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OK, bad = 0, 0\n",
    "data=[]\n",
    "tmp_idx=-1\n",
    "for idx in df_clustered.index:\n",
    "    \n",
    "    if tmp_idx >= 5000:\n",
    "        af2_path = '/home/nfs/sdunin/tmp/hamp'\n",
    "    else:\n",
    "        af2_path = '/home/nfs/rmadaj/hamp/HAMPpred/clustering/af2_structures'\n",
    "\n",
    "    \n",
    "    row = df_clustered.loc[idx]\n",
    "    group = row['id'].split(\"|\")[1]\n",
    "    true_id = int(row['id'].split(\"|\")[0])\n",
    "        \n",
    "    # aligned sequence\n",
    "    alnseq = hampid2alnseq.loc[idx].sequence\n",
    "    \n",
    "    # cut helix1 and helix2\n",
    "    h1 = alnseq[h1_msa_start:h1_msa_stop].replace('-', '')\n",
    "    h2 = alnseq[h2_msa_start:h2_msa_stop].replace('-', '')\n",
    "    \n",
    "    if len(h1) != len(h2): \n",
    "        bad+=1\n",
    "        continue\n",
    "        \n",
    "    seq = row.sequence    \n",
    "    if seq.find('X')>-1: \n",
    "        bad+=1\n",
    "        continue\n",
    "        \n",
    "    OK+=1\n",
    "        \n",
    "    continue\n",
    "\n",
    "    # measure model        \n",
    "    h1_start = seq.find(h1); assert h1_start > -1\n",
    "    h2_start = seq.find(h2); assert h2_start > -1\n",
    "    \n",
    "    a1_start, a1_stop = h1_start+1, h1_start+len(h1)+1\n",
    "    a2_start, a2_stop = h2_start+1, h2_start+len(h2)+1\n",
    "    chain1, chain2 = 'A', 'B'\n",
    "    \n",
    "    kwargs = {'a1_start':a1_start, \n",
    "              'a1_stop':a1_stop,\n",
    "              'a2_start':a2_start,\n",
    "              'a2_stop':a2_stop,\n",
    "              'chain1':chain1,\n",
    "              'chain2':chain2}\n",
    "    \n",
    "    \n",
    "    \n",
    "    mapfunc = partial(measure_one_HAMP, **kwargs)\n",
    "    \n",
    "    # get 1 rank af2 model\n",
    "    # old run\n",
    "    # hamp_315_A_group_44_unrelaxed_rank_001_alphafold2_multimer_v3_model_5_seed_000.pdb\n",
    "\n",
    "    #pdb_files = sorted(glob(os.path.join(af2_path, str(idx), '*_relaxed*.pdb')), key=lambda x:int(x.split('/')[-1].split(\"_\")[4]))\n",
    "    \n",
    "    tmp_idx +=1 \n",
    "    \n",
    "    #tmp_idx = df_clustered.index.get_loc(idx)\n",
    "    #print(idx, tmp_idx)\n",
    "    \n",
    "    pdb_files = glob(os.path.join(af2_path, str(tmp_idx), '*_unrelaxed*rank_001*.pdb'))\n",
    "    \n",
    "    # analyse only rank 1 model\n",
    "    pdb_files = [pdb_files[0]]\n",
    "        \n",
    "    # analyse selected models \n",
    "    for job, pdb_file in zip(run_multiprocess(mapfunc, pdb_files, len(pdb_files)), pdb_files):    \n",
    "                \n",
    "        dir_path, filename = os.path.split(pdb_file)\n",
    "        _, last_dir = os.path.split(dir_path)\n",
    "        assert tmp_idx == int(last_dir)\n",
    "        pdb_file_simple = os.path.join(last_dir, filename)\n",
    "        \n",
    "        # parse scores 'max_pae', 'pae', 'plddt', 'ptm'\n",
    "        json_file = pdb_file[:-4].replace('_unrelaxed_', '_scores_') + \".json\"\n",
    "        scores = json.load(open(json_file))\n",
    "        \n",
    "        # measure with SamCC\n",
    "        \n",
    "        bundle_df, n_crick_mut, c_crick_mut = job\n",
    "        \n",
    "        #    crick = bundle_df.crick.values # in measure_one_HAMP\n",
    "        #    n_crick = crick[0::2]\n",
    "        #    c_crick = crick[1::2]\n",
    "        \n",
    "        \n",
    "        n_shift = bundle_df['shift'][0::2].mean()\n",
    "        c_shift = bundle_df['shift'][1::2].mean()\n",
    "        \n",
    "        n_radius = bundle_df['radius'][0::2].mean()\n",
    "        c_radius = bundle_df['radius'][1::2].mean()\n",
    "        \n",
    "        n_A = bundle_df['A'][0::2].mean()\n",
    "        c_A = bundle_df['A'][1::2].mean()\n",
    "        \n",
    "        nn_P = bundle_df['P'][0::2].mean()\n",
    "        cc_P = bundle_df['P'][1::2].mean()\n",
    "        \n",
    "        n_crick_mut = n_crick_mut[2:-2]\n",
    "        c_crick_mut = c_crick_mut[2:-2]\n",
    "        \n",
    "        # assume canonical bundle periodicity for calculating reference Crick angles\n",
    "        n_P = c_P = 3.5 \n",
    "\n",
    "        c_phi = n_phi = crangles[start_hep] \n",
    "\n",
    "        n_crick_ref = get_ref_crick(n_P, n_phi)[:len(n_crick_mut)]\n",
    "        c_crick_ref = get_ref_crick(c_P, c_phi)[:len(c_crick_mut)]\n",
    "\n",
    "        n_crick_diff = diffangle(n_crick_mut, n_crick_ref)\n",
    "        n_crick_diff = (n_crick_diff[0::2] + n_crick_diff[1::2])/2\n",
    "\n",
    "        c_crick_diff = diffangle(c_crick_mut, c_crick_ref)\n",
    "        c_crick_diff = (c_crick_diff[0::2] + c_crick_diff[1::2])/2\n",
    "\n",
    "        n_crick_diff = np.mean(n_crick_diff)\n",
    "        c_crick_diff = np.mean(c_crick_diff)\n",
    "\n",
    "        # rotation asymmetry\n",
    "        crick_diff = diffangle(n_crick_diff, c_crick_diff) \n",
    "\n",
    "        seq1 = bundle_df.res_name[0::4].tolist()\n",
    "        seq2 = bundle_df.res_name[1::4].tolist()\n",
    "        seq1 = \"\".join([AA_3_to_1[res] for res in seq1])\n",
    "        seq2 = \"\".join([AA_3_to_1[res] for res in seq2])\n",
    "        \n",
    "        assert seq1 == h1 and seq2 == h2\n",
    "\n",
    "        # add record\n",
    "        \n",
    "        print(h1, h2, idx, tmp_idx, group)\n",
    "        \n",
    "        data.append(\n",
    "                (true_id, group, n_crick_diff, c_crick_diff, crick_diff, \\\n",
    "                 h1, h2, n_crick_mut, c_crick_mut, seq, pdb_file_simple,\n",
    "                 scores['max_pae'], np.mean(scores['plddt']), scores['ptm'], np.mean(scores['pae']),\n",
    "                 n_shift, c_shift, n_radius, c_radius, n_A, c_A, nn_P, cc_P\n",
    "                )\n",
    "            )\n",
    "        \n",
    "    #if len(data)>10:break\n",
    "        \n",
    "    # debug\n",
    "    #break\n",
    "        \n",
    "data_df = pd.DataFrame(data, columns=['idx', 'group', 'n_rot', 'c_rot', 'rot', \n",
    "                                      'n_seq', 'c_seq', 'n_crick_mut', 'c_crick_mut', 'full_seq', 'pdb_file',\n",
    "                                      'max_pae', 'plddt', 'ptm', 'pae_mean', \n",
    "                                      'n_shift', 'c_shift', 'n_radius', 'c_radius', 'n_A', 'c_A', 'n_P', 'c_P'])\n",
    "assert data_df['idx'].is_unique\n",
    "data_df.set_index('idx', inplace=True)\n",
    "len(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae69551c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5388"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OK+bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c052f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfee504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_pickle(os.path.join(data_dir, 'af2_newrun.p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20155800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checked, OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b28a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
